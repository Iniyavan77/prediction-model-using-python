# -*- coding: utf-8 -*-
"""MDS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QWtt06uRHClOt4kXLQ1LpjZWVMR_egtF
"""

#TASK 1
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt6
data = {
 'Years of Experience': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
 'Qualification': ['Bachelors', 'Bachelors', 'Masters', 'Masters',
'PhD', 'PhD', 'Masters', 'Bachelors', 'Masters', 'PhD'],
 'Salary': [45000, 48000, 52000, 60000, 70000, 75000, 68000, 49000,
65000, 80000]
}
df = pd.DataFrame(data)
df = pd.get_dummies(df, columns=['Qualification'], drop_first=True)
X = df.drop('Salary', axis=1)
y = df['Salary']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
model = LinearRegression()
model.fit(X_train_scaled, y_train)
y_pred = model.predict(X_test_scaled)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
plt.scatter(y_test, y_pred)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)],
color='red', lw=2)
plt.xlabel('Actual Salary')
plt.ylabel('Predicted Salary')
plt.title('Actual vs Predicted Sales')
plt.show()

#TASK 2
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder
data = {
    'Height': [160, 170, 180, 165, 155, 190, 175, 168],
    'Weight': [60, 70, 80, 65, 55, 90, 75, 68],
    'Gender': ['Male', 'Female', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male'],
    'Age': [25, 30, 35, 28, 22, 40, 33, 27]
}
df = pd.DataFrame(data)
label_encoder = LabelEncoder()
df['Gender'] = label_encoder.fit_transform(df['Gender'])
X = df[['Height', 'Weight', 'Gender']]
y = df['Age']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Model Coefficients:", model.coef_)
print("Model Intercept:", model.intercept_)
print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("R-squared (R²):", r2)
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.scatter(df['Height'], df['Age'], color='blue', label='Height vs Age')
plt.title('Height vs Age')
plt.xlabel('Height (cm)')
plt.ylabel('Age')
plt.grid(True)
plt.subplot(1, 2, 2)
plt.scatter(df['Weight'], df['Age'], color='green', label='Weight vs Age')
plt.title('Weight vs Age')
plt.xlabel('Weight (kg)')
plt.ylabel('Age')
plt.grid(True)
plt.tight_layout()
plt.show()
plt.figure(figsize=(8, 6))
plt.plot(y_test.values, color='blue', marker='o', label='Actual Age')
plt.plot(y_pred, color='red', marker='x', linestyle='dashed', label='Predicted Age')
plt.title('Actual vs Predicted Age')
plt.xlabel('Test Sample Index')
plt.ylabel('Age')
plt.legend()
plt.grid(True)
plt.show()
new_data = pd.DataFrame({'Height': [172], 'Weight': [68], 'Gender': [label_encoder.transform(['Female'])[0]]})
predicted_age = model.predict(new_data)
print("Predicted Age for the new data:", predicted_age[0])

#TASK 3
import numpy as np
import matplotlib.pyplot as plt
x = np.array([1, 2, 3, 4, 5])
y = np.array([1.2, 2.3, 3.1, 4.5, 5.3])
X = np.vstack([np.ones(len(x)), x]).T
XT = X.T
beta = np.linalg.inv(XT @ X) @ XT @ y
beta_0, beta_1 = beta
print(f"Intercept (beta_0): {beta_0}")
print(f"Slope (beta_1): {beta_1}")
y_pred = X @ beta
plt.scatter(x, y, color='blue', label='Data Points')
plt.plot(x, y_pred, color='red', label=f'Least Squares Fit: y = {beta_0:.2f} + {beta_1:.2f}x')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()

#TASK 4A
import numpy as np
import matplotlib.pyplot as plt
x = np.array([1, 2, 3, 4, 5])
y = np.array([2.3, 2.9, 3.5, 4.6, 5.1])
X = np.vstack([np.ones(len(x)), x]).T
beta = np.linalg.inv(X.T @ X) @ X.T @ y
beta_0, beta_1 = beta
print(f"Intercept (beta_0): {beta_0}")
print(f"Slope (beta_1): {beta_1}")
y_pred = X @ beta
plt.scatter(x, y, color='blue', label='Data Points')
plt.plot(x, y_pred, color='red', label=f'Best Fit Line: y = {beta_0:.2f} + {beta_1:.2f}x')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()

#TASK 4B
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
data = {
'x1': [1, 2, 3, 4, 5],
'x2': [5, 4, 3, 2, 1],
'y': [7.2, 6.9, 6.5, 5.7, 5.1]
}
df = pd.DataFrame(data)
X = df[['x1', 'x2']].values
y = df['y'].values
model = LinearRegression()
model.fit(X, y)
beta_0 = model.intercept_
beta_1, beta_2 = model.coef_
print(f"Intercept (beta_0): {beta_0}")
print(f"Slope (beta_1) for x1: {beta_1}")
print(f"Slope (beta_2) for x2: {beta_2}")
y_pred = model.predict(X)
plt.scatter(y, y_pred, color='blue')
plt.plot([min(y), max(y)], [min(y), max(y)], color='red', linestyle='--')
plt.xlabel('Actual y')
plt.ylabel('Predicted y')
plt.title('Actual vs Predicted values')
plt.show()

#TASK 5
import numpy as np
from scipy import stats

group1 = np.array([23, 45, 34, 33, 27, 30, 35])
group2 = np.array([40, 44, 35, 38, 42, 45, 39])

t_stat, p_value = stats.ttest_ind(group1, group2)

print(f"t-statistic: {t_stat}")
print(f"p-value: {p_value}")

if p_value < 0.05:
    print("Reject the null hypothesis: The means are significantly different.")
else:
    print("Fail to reject the null hypothesis: The means are equal.")

#TaSK 6
import numpy as np

# Instead of set.seed(), use np.random.seed()
np.random.seed(123)

# Generate two sample datasets using numpy
group1 = np.random.normal(50, 10, 30)  # Sample 1
group2 = np.random.normal(55, 15, 30)  # Sample 2

# Compute Variances
var1 = np.var(group1)
var2 = np.var(group2)

# Perform F-Test using scipy.stats
from scipy import stats
f_test = stats.f_oneway(group1, group2)

# Compute Ratio (Larger variance / Smaller variance)
variance_ratio = max(var1, var2) / min(var1, var2)

# Display Results
print("group1")
print(group1)
print("group2")
print(group2)
print(f_test)
print("Ratio of Larger Variance to Smaller Variance:", variance_ratio, "\n")

#TASK 7
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
# Load the dataset
from seaborn import load_dataset
cars = pd.DataFrame({'speed': [4, 7, 8, 9, 10, 15, 20, 25, 30, 35],
'dist': [2, 10, 4, 22, 16, 26, 34, 48, 60, 76]})
# Data Visualization
plt.scatter(cars['speed'], cars['dist'], color='blue')
plt.xlabel('Speed (mph)')
plt.ylabel('Stopping Distance (ft)')
plt.title('Speed vs Stopping Distance')
plt.show()
# Split dataset into training (80%) and testing (20%)
X = cars[['speed']]
y = cars['dist']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Train the Linear Regression Model
model = LinearRegression()
model.fit(X_train, y_train)
# Model Evaluation
y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)
print(f"RMSE: {rmse:.2f}")
print(f"R-squared: {r2:.2f}")
# Plot Regression Line
plt.scatter(X_train, y_train, color='blue', label='Actual Data')
plt.plot(X_train, model.predict(X_train), color='red', linewidth=2, label='Regression Line')
plt.xlabel('Speed (mph)')
plt.ylabel('Stopping Distance (ft)')
plt.title('Linear Regression Model')
plt.legend()
plt.show()

#USE CASE

# Create the dataset
fish_mercury_data <- data.frame(
  Fisherman_ID = 1:10,
  Fish_Meals_Per_Week = c(2, 3, 1, 5, 4, 6, 7, 2, 8, 3),
  Mercury_Levels = c(5.3, 6.1, 4.1, 7.2, 6.8, 8.4, 9.0, 5.4, 9.5, 6.3)
)

# View the dataset
print(fish_mercury_data)

# Fit the linear regression model
model <- lm(Mercury_Levels ~ Fish_Meals_Per_Week, data = fish_mercury_data)

# View the summary of the model
summary(model)

# Plot the data and add the regression line
plot(fish_mercury_data$Fish_Meals_Per_Week, fish_mercury_data$Mercury_Levels,
     xlab = "Fish Meals Per Week", ylab = "Mercury Levels (µg/L)",
     main = "Scatter Plot of Fish Meals vs Mercury Levels", pch = 19, col = "blue")
abline(model, col = "red")

# Diagnostic plots to check assumptions
par(mfrow = c(2, 2))  # Arrange the plots in 2x2 layout
plot(model)

#TASK 8
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
# Load the training dataset
train_data = pd.DataFrame({
'Experience': [1, 2, 3, 4, 5, 6],
'Salary': [3000, 3500, 4000, 4500, 5000, 5500]
})
# Split into X (features) and y (target)
X_train = train_data[['Experience']]
y_train = train_data['Salary']
# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)
# Load the new dataset (without salary)
new_data = pd.DataFrame({'Experience': [2.5, 4.5, 5.5]})
# Make predictions
predictions = model.predict(new_data)
# Display predictions
new_data['Predicted Salary'] = predictions
print(new_data)

#TASK 9
import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats

# Load the dataset
url = "https://raw.githubusercontent.com/selva86/datasets/master/mtcars.csv"
df = pd.read_csv(url)

# Selecting relevant variables
X = df[['hp', 'wt', 'disp']]  # Independent variables
y = df['mpg']  # Dependent variable

# Add constant for intercept
X = sm.add_constant(X)

# Fit regression model
model = sm.OLS(y, X).fit()

# Generate diagnostic plots
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# 1. Residuals vs. Fitted
sns.residplot(x=model.fittedvalues, y=model.resid, lowess=True, ax=axes[0, 0], line_kws={'color': 'red'})
axes[0, 0].set_title('Residuals vs Fitted')
axes[0, 0].set_xlabel('Fitted Values')
axes[0, 0].set_ylabel('Residuals')

# 2. Normal Q-Q Plot
sm.qqplot(model.resid, line='45', ax=axes[0, 1])
axes[0, 1].set_title('Normal Q-Q Plot')

# 3. Scale-Location Plot
sqrt_residuals = np.sqrt(np.abs(model.resid))
sns.scatterplot(x=model.fittedvalues, y=sqrt_residuals, ax=axes[1, 0])
axes[1, 0].set_title('Scale-Location Plot')
axes[1, 0].set_xlabel('Fitted Values')
axes[1, 0].set_ylabel('√ |Residuals|')

# 4. Residuals vs Leverage
sm.graphics.influence_plot(model, ax=axes[1, 1], criterion="cooks")

plt.tight_layout()
plt.show()

# Print model summary
print(model.summary())

#TASK 10
import pandas as pd
import statsmodels.api as sm
from statsmodels.stats.stattools import durbin_watson # Changed staƩools to stattools
df = pd.read_csv("sales_data.csv")
X = df[['AdverƟsing_Spend']]
y = df['Sales']
X = sm.add_constant(X)
model = sm.OLS(y, X).fit()
dw_staƟsƟc = durbin_watson(model.resid)
print(f"Durbin-Watson StaƟsƟc: {dw_staƟsƟc:.3f}")
import matplotlib.pyplot as plt
plt.plot(model.resid, marker='o', linestyle='-', color='blue')
plt.axhline(y=0, color='red', linestyle='dashed')
plt.title("Residual Plot")
plt.xlabel("ObservaƟon")
plt.ylabel("Residuals")
plt.show()

